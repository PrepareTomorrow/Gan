{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic GAN 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tqdm\n",
    "#!conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)\n",
    "- [NIPS 2016 Tutorial:\n",
    "Generative Adversarial Networks](https://arxiv.org/pdf/1701.00160.pdf)\n",
    "- [image source](https://xiaohongliu.ca/post/gan/)\n",
    "![gan2-2.PNG](attachment:gan2-2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "학습에 사용될 hyperparameter 값들을 넣을 class를 정의합니다.\n",
    "\"\"\"\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GAN model 학습에 사용되는 결과 이미지 저장 경로, 에포크 수, 모델 입력 이미지 크기 등을 정의합니다.\n",
    "\"\"\"\n",
    "config = AttrDict()\n",
    "config.data_path = 'data/'\n",
    "config.save_path = 'save/'\n",
    "config.dataset = 'CIFAR10' #CIFAR10\n",
    "config.n_epoch = 500\n",
    "config.log_interval = 100\n",
    "config.save_interval = 20\n",
    "config.batch_size = 64\n",
    "config.learning_rate = 0.0002\n",
    "config.b1 = 0.5\n",
    "config.b2 = 0.999\n",
    "config.img_shape = (3, 32, 32)\n",
    "config.latent_size = 100\n",
    "\"\"\"\n",
    "모델 입력 이미지에 수행할 normalization과 모델 생성 결과 이미지에 수행할 denormalization을 정의합니다.\n",
    "\"\"\"\n",
    "config.augmentation = transforms.Compose([\n",
    "                        transforms.Resize((config.img_shape[1], config.img_shape[2])),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(mean=[0.5], std=[0.5]) \n",
    "                      ])\n",
    "config.denormalize = lambda x: x*0.5+0.5\n",
    "config.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(config.data_path):\n",
    "    os.makedirs(config.data_path)\n",
    "if not os.path.isdir(os.path.join(config.save_path, config.dataset)):\n",
    "    os.makedirs(os.path.join(config.save_path, config.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#torch.cuda.current_device()\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]\n",
    "#torch.cuda_path = r'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3'\n",
    "torch.cuda.is_available()\n",
    "#torch.cuda_version\n",
    "#torch.cuda.get_device_name(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: data/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.5], std=[0.5])\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MNIST와 CIFAR-10은 torchvision 라이브러리에서 제공하여 아래와 같이 사용할 수 있습니다.\n",
    "\"\"\"\n",
    "if config.dataset == 'MNIST':\n",
    "    train_dataset = datasets.MNIST(config.data_path,\n",
    "                                    train=True,\n",
    "                                    download=True,\n",
    "                                    transform=config.augmentation\n",
    "                                  ) \n",
    "elif config.dataset == 'CIFAR10': \n",
    "    train_dataset = datasets.CIFAR10(config.data_path,\n",
    "                                       train=True,\n",
    "                                       download=True,\n",
    "                                       transform=config.augmentation\n",
    "                                     )\n",
    "\"\"\"\n",
    "training set을 Dataloader에 넣습니다. \n",
    "\"\"\"\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GAN model\n",
    "\"\"\"\n",
    " 일반적으로, GAN에서는 loss가 Discriminator에서부터 Generator로 흐를 때 생길 수 있는 \n",
    " vanishing gradient 현상을 완화하기 위해 Leaky ReLU를 많이 사용합니다. \n",
    "\"\"\"\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *self.block(config.latent_size, 128, batchnorm=False),\n",
    "            *self.block(128, 256),\n",
    "            *self.block(256, 512),\n",
    "            *self.block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(config.img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.reshape(img.shape[0], *config.img_shape)\n",
    "        return img\n",
    "    \n",
    "    def block(self, input_size, output_size, batchnorm=True):\n",
    "        layers = [nn.Linear(input_size, output_size)]\n",
    "        if batchnorm:\n",
    "            layers.append(nn.BatchNorm1d(output_size))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(config.img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img = img.reshape(img.shape[0], -1)\n",
    "        validity = self.model(img)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Cross Entropy loss between the target and the input probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [torch.nn.BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bceloss.PNG](attachment:bceloss.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "binary cross entropy loss를 사용하여 adversarial loss를 구현합니다.\n",
    "\"\"\"\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\"\"\"\n",
    "Generator와 Discriminator를 각각 정의하고, 상응하는 optimizer도 함께 정의합니다.\n",
    "\"\"\"\n",
    "generator = Generator(config).to(config.device)\n",
    "discriminator = Discriminator(config).to(config.device)\n",
    "\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), lr=config.learning_rate, betas=(config.b1, config.b2))\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=config.learning_rate, betas=(config.b1, config.b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "  (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (5): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (8): Linear(in_features=512, out_features=1024, bias=True)\n",
       "  (9): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (11): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "  (12): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3072, out_features=512, bias=True)\n",
       "  (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (5): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf71dbf867fd4acc9621988e2a5d8601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] Batch [100/782] Discriminator loss: 0.5269 Generator loss: 1.8016\n",
      "Epoch [1/500] Batch [200/782] Discriminator loss: 0.5587 Generator loss: 1.8023\n",
      "Epoch [1/500] Batch [300/782] Discriminator loss: 0.5040 Generator loss: 1.8120\n",
      "Epoch [1/500] Batch [400/782] Discriminator loss: 0.4065 Generator loss: 2.6937\n",
      "Epoch [1/500] Batch [500/782] Discriminator loss: 0.4722 Generator loss: 2.4059\n",
      "Epoch [1/500] Batch [600/782] Discriminator loss: 0.5386 Generator loss: 1.6578\n",
      "Epoch [1/500] Batch [700/782] Discriminator loss: 0.5256 Generator loss: 2.0802\n",
      "Epoch [2/500] Batch [100/782] Discriminator loss: 0.7006 Generator loss: 1.9939\n",
      "Epoch [2/500] Batch [200/782] Discriminator loss: 0.7102 Generator loss: 1.5617\n",
      "Epoch [2/500] Batch [300/782] Discriminator loss: 0.5364 Generator loss: 1.7269\n",
      "Epoch [2/500] Batch [400/782] Discriminator loss: 0.5203 Generator loss: 1.6339\n",
      "Epoch [2/500] Batch [500/782] Discriminator loss: 0.5752 Generator loss: 1.3604\n",
      "Epoch [2/500] Batch [600/782] Discriminator loss: 0.5750 Generator loss: 1.6913\n",
      "Epoch [2/500] Batch [700/782] Discriminator loss: 0.6414 Generator loss: 1.4713\n",
      "Epoch [3/500] Batch [100/782] Discriminator loss: 0.5652 Generator loss: 1.1491\n",
      "Epoch [3/500] Batch [200/782] Discriminator loss: 0.6716 Generator loss: 1.1771\n",
      "Epoch [3/500] Batch [300/782] Discriminator loss: 0.6168 Generator loss: 1.4515\n",
      "Epoch [3/500] Batch [400/782] Discriminator loss: 0.5696 Generator loss: 1.0804\n",
      "Epoch [3/500] Batch [500/782] Discriminator loss: 0.6522 Generator loss: 1.2538\n",
      "Epoch [3/500] Batch [600/782] Discriminator loss: 0.5904 Generator loss: 1.0853\n",
      "Epoch [3/500] Batch [700/782] Discriminator loss: 0.5755 Generator loss: 1.1074\n",
      "Epoch [4/500] Batch [100/782] Discriminator loss: 0.5929 Generator loss: 1.1362\n",
      "Epoch [4/500] Batch [200/782] Discriminator loss: 0.7091 Generator loss: 1.1547\n",
      "Epoch [4/500] Batch [300/782] Discriminator loss: 0.5744 Generator loss: 1.2015\n",
      "Epoch [4/500] Batch [400/782] Discriminator loss: 0.6277 Generator loss: 1.0846\n",
      "Epoch [4/500] Batch [500/782] Discriminator loss: 0.6485 Generator loss: 1.1194\n",
      "Epoch [4/500] Batch [600/782] Discriminator loss: 0.6117 Generator loss: 0.9602\n",
      "Epoch [4/500] Batch [700/782] Discriminator loss: 0.6989 Generator loss: 0.9508\n",
      "Epoch [5/500] Batch [100/782] Discriminator loss: 0.5971 Generator loss: 1.1251\n",
      "Epoch [5/500] Batch [200/782] Discriminator loss: 0.6196 Generator loss: 1.0397\n",
      "Epoch [5/500] Batch [300/782] Discriminator loss: 0.6855 Generator loss: 1.1975\n",
      "Epoch [5/500] Batch [400/782] Discriminator loss: 0.6958 Generator loss: 0.9006\n",
      "Epoch [5/500] Batch [500/782] Discriminator loss: 0.6934 Generator loss: 0.9272\n",
      "Epoch [5/500] Batch [600/782] Discriminator loss: 0.6400 Generator loss: 0.9111\n",
      "Epoch [5/500] Batch [700/782] Discriminator loss: 0.6269 Generator loss: 0.9847\n",
      "Epoch [6/500] Batch [100/782] Discriminator loss: 0.6792 Generator loss: 0.9735\n",
      "Epoch [6/500] Batch [200/782] Discriminator loss: 0.6370 Generator loss: 1.0325\n",
      "Epoch [6/500] Batch [300/782] Discriminator loss: 0.6416 Generator loss: 0.9613\n",
      "Epoch [6/500] Batch [400/782] Discriminator loss: 0.7255 Generator loss: 0.9255\n",
      "Epoch [6/500] Batch [500/782] Discriminator loss: 0.6795 Generator loss: 0.9239\n",
      "Epoch [6/500] Batch [600/782] Discriminator loss: 0.6515 Generator loss: 0.8999\n",
      "Epoch [6/500] Batch [700/782] Discriminator loss: 0.6746 Generator loss: 0.9422\n",
      "Epoch [7/500] Batch [100/782] Discriminator loss: 0.6767 Generator loss: 0.9580\n",
      "Epoch [7/500] Batch [200/782] Discriminator loss: 0.6095 Generator loss: 0.9069\n",
      "Epoch [7/500] Batch [300/782] Discriminator loss: 0.6799 Generator loss: 0.9486\n",
      "Epoch [7/500] Batch [400/782] Discriminator loss: 0.6839 Generator loss: 0.8904\n",
      "Epoch [7/500] Batch [500/782] Discriminator loss: 0.6656 Generator loss: 0.9250\n",
      "Epoch [7/500] Batch [600/782] Discriminator loss: 0.7020 Generator loss: 0.8788\n",
      "Epoch [7/500] Batch [700/782] Discriminator loss: 0.6385 Generator loss: 0.8209\n",
      "Epoch [8/500] Batch [100/782] Discriminator loss: 0.6452 Generator loss: 0.8815\n",
      "Epoch [8/500] Batch [200/782] Discriminator loss: 0.7089 Generator loss: 0.8909\n",
      "Epoch [8/500] Batch [300/782] Discriminator loss: 0.6554 Generator loss: 0.9487\n",
      "Epoch [8/500] Batch [400/782] Discriminator loss: 0.6898 Generator loss: 0.8567\n",
      "Epoch [8/500] Batch [500/782] Discriminator loss: 0.6539 Generator loss: 0.8358\n",
      "Epoch [8/500] Batch [600/782] Discriminator loss: 0.6914 Generator loss: 0.8750\n",
      "Epoch [8/500] Batch [700/782] Discriminator loss: 0.6559 Generator loss: 0.8680\n",
      "Epoch [9/500] Batch [100/782] Discriminator loss: 0.6862 Generator loss: 0.8776\n",
      "Epoch [9/500] Batch [200/782] Discriminator loss: 0.6611 Generator loss: 0.8726\n",
      "Epoch [9/500] Batch [300/782] Discriminator loss: 0.6825 Generator loss: 0.8306\n",
      "Epoch [9/500] Batch [400/782] Discriminator loss: 0.6864 Generator loss: 0.8740\n",
      "Epoch [9/500] Batch [500/782] Discriminator loss: 0.6573 Generator loss: 0.8530\n",
      "Epoch [9/500] Batch [600/782] Discriminator loss: 0.7008 Generator loss: 0.8687\n",
      "Epoch [9/500] Batch [700/782] Discriminator loss: 0.6568 Generator loss: 0.9105\n",
      "Epoch [10/500] Batch [100/782] Discriminator loss: 0.6389 Generator loss: 0.8325\n",
      "Epoch [10/500] Batch [200/782] Discriminator loss: 0.7296 Generator loss: 0.8555\n",
      "Epoch [10/500] Batch [300/782] Discriminator loss: 0.6595 Generator loss: 0.8511\n",
      "Epoch [10/500] Batch [400/782] Discriminator loss: 0.6964 Generator loss: 0.8418\n",
      "Epoch [10/500] Batch [500/782] Discriminator loss: 0.6779 Generator loss: 0.8211\n",
      "Epoch [10/500] Batch [600/782] Discriminator loss: 0.6626 Generator loss: 0.9076\n",
      "Epoch [10/500] Batch [700/782] Discriminator loss: 0.6919 Generator loss: 0.8217\n",
      "Epoch [11/500] Batch [100/782] Discriminator loss: 0.6594 Generator loss: 0.8273\n",
      "Epoch [11/500] Batch [200/782] Discriminator loss: 0.6492 Generator loss: 0.8116\n",
      "Epoch [11/500] Batch [300/782] Discriminator loss: 0.6442 Generator loss: 0.8235\n",
      "Epoch [11/500] Batch [400/782] Discriminator loss: 0.6761 Generator loss: 0.8541\n",
      "Epoch [11/500] Batch [500/782] Discriminator loss: 0.6635 Generator loss: 0.8244\n",
      "Epoch [11/500] Batch [600/782] Discriminator loss: 0.7124 Generator loss: 0.7896\n",
      "Epoch [11/500] Batch [700/782] Discriminator loss: 0.7134 Generator loss: 0.9009\n",
      "Epoch [12/500] Batch [100/782] Discriminator loss: 0.6605 Generator loss: 0.8218\n",
      "Epoch [12/500] Batch [200/782] Discriminator loss: 0.6696 Generator loss: 0.8557\n",
      "Epoch [12/500] Batch [300/782] Discriminator loss: 0.6999 Generator loss: 0.7724\n",
      "Epoch [12/500] Batch [400/782] Discriminator loss: 0.6533 Generator loss: 0.7573\n",
      "Epoch [12/500] Batch [500/782] Discriminator loss: 0.6691 Generator loss: 0.7946\n",
      "Epoch [12/500] Batch [600/782] Discriminator loss: 0.6284 Generator loss: 0.8492\n",
      "Epoch [12/500] Batch [700/782] Discriminator loss: 0.6472 Generator loss: 0.7686\n",
      "Epoch [13/500] Batch [100/782] Discriminator loss: 0.6655 Generator loss: 0.8194\n",
      "Epoch [13/500] Batch [200/782] Discriminator loss: 0.6784 Generator loss: 0.7291\n",
      "Epoch [13/500] Batch [300/782] Discriminator loss: 0.7118 Generator loss: 0.7864\n",
      "Epoch [13/500] Batch [400/782] Discriminator loss: 0.6927 Generator loss: 0.8228\n",
      "Epoch [13/500] Batch [500/782] Discriminator loss: 0.6778 Generator loss: 0.7892\n",
      "Epoch [13/500] Batch [600/782] Discriminator loss: 0.7148 Generator loss: 0.8535\n",
      "Epoch [13/500] Batch [700/782] Discriminator loss: 0.6709 Generator loss: 0.7233\n",
      "Epoch [14/500] Batch [100/782] Discriminator loss: 0.7041 Generator loss: 0.8061\n",
      "Epoch [14/500] Batch [200/782] Discriminator loss: 0.6614 Generator loss: 0.8821\n",
      "Epoch [14/500] Batch [300/782] Discriminator loss: 0.6700 Generator loss: 0.7898\n",
      "Epoch [14/500] Batch [400/782] Discriminator loss: 0.7013 Generator loss: 0.7966\n",
      "Epoch [14/500] Batch [500/782] Discriminator loss: 0.6794 Generator loss: 0.8771\n",
      "Epoch [14/500] Batch [600/782] Discriminator loss: 0.6699 Generator loss: 0.7649\n",
      "Epoch [14/500] Batch [700/782] Discriminator loss: 0.7028 Generator loss: 0.7624\n",
      "Epoch [15/500] Batch [100/782] Discriminator loss: 0.6464 Generator loss: 0.8914\n",
      "Epoch [15/500] Batch [200/782] Discriminator loss: 0.6947 Generator loss: 0.7245\n",
      "Epoch [15/500] Batch [300/782] Discriminator loss: 0.6981 Generator loss: 0.8355\n",
      "Epoch [15/500] Batch [400/782] Discriminator loss: 0.6396 Generator loss: 0.7829\n",
      "Epoch [15/500] Batch [500/782] Discriminator loss: 0.6944 Generator loss: 0.9457\n",
      "Epoch [15/500] Batch [600/782] Discriminator loss: 0.6894 Generator loss: 0.7339\n",
      "Epoch [15/500] Batch [700/782] Discriminator loss: 0.7201 Generator loss: 0.6916\n",
      "Epoch [16/500] Batch [100/782] Discriminator loss: 0.6857 Generator loss: 0.8384\n",
      "Epoch [16/500] Batch [200/782] Discriminator loss: 0.6648 Generator loss: 0.7612\n",
      "Epoch [16/500] Batch [300/782] Discriminator loss: 0.7363 Generator loss: 0.6913\n",
      "Epoch [16/500] Batch [400/782] Discriminator loss: 0.6477 Generator loss: 0.7955\n",
      "Epoch [16/500] Batch [500/782] Discriminator loss: 0.7206 Generator loss: 0.7405\n",
      "Epoch [16/500] Batch [600/782] Discriminator loss: 0.6757 Generator loss: 0.7758\n",
      "Epoch [16/500] Batch [700/782] Discriminator loss: 0.7041 Generator loss: 0.7712\n",
      "Epoch [17/500] Batch [100/782] Discriminator loss: 0.7210 Generator loss: 0.8209\n",
      "Epoch [17/500] Batch [200/782] Discriminator loss: 0.6313 Generator loss: 0.7136\n",
      "Epoch [17/500] Batch [300/782] Discriminator loss: 0.6627 Generator loss: 0.7663\n",
      "Epoch [17/500] Batch [400/782] Discriminator loss: 0.6768 Generator loss: 0.7927\n",
      "Epoch [17/500] Batch [500/782] Discriminator loss: 0.7006 Generator loss: 0.7875\n",
      "Epoch [17/500] Batch [600/782] Discriminator loss: 0.6706 Generator loss: 0.7264\n",
      "Epoch [17/500] Batch [700/782] Discriminator loss: 0.6894 Generator loss: 0.7579\n",
      "Epoch [18/500] Batch [100/782] Discriminator loss: 0.7256 Generator loss: 0.7512\n",
      "Epoch [18/500] Batch [200/782] Discriminator loss: 0.6973 Generator loss: 0.8187\n",
      "Epoch [18/500] Batch [300/782] Discriminator loss: 0.6508 Generator loss: 0.8489\n",
      "Epoch [18/500] Batch [400/782] Discriminator loss: 0.6686 Generator loss: 0.8077\n",
      "Epoch [18/500] Batch [500/782] Discriminator loss: 0.7111 Generator loss: 0.7576\n",
      "Epoch [18/500] Batch [600/782] Discriminator loss: 0.6997 Generator loss: 0.7491\n",
      "Epoch [18/500] Batch [700/782] Discriminator loss: 0.6857 Generator loss: 0.7804\n",
      "Epoch [19/500] Batch [100/782] Discriminator loss: 0.6885 Generator loss: 0.7323\n",
      "Epoch [19/500] Batch [200/782] Discriminator loss: 0.7078 Generator loss: 0.6816\n",
      "Epoch [19/500] Batch [300/782] Discriminator loss: 0.6941 Generator loss: 0.8033\n",
      "Epoch [19/500] Batch [400/782] Discriminator loss: 0.6860 Generator loss: 0.8011\n",
      "Epoch [19/500] Batch [500/782] Discriminator loss: 0.6776 Generator loss: 0.7512\n",
      "Epoch [19/500] Batch [600/782] Discriminator loss: 0.6878 Generator loss: 0.8235\n",
      "Epoch [19/500] Batch [700/782] Discriminator loss: 0.6804 Generator loss: 0.7905\n",
      "Epoch [20/500] Batch [100/782] Discriminator loss: 0.6912 Generator loss: 0.7829\n",
      "Epoch [20/500] Batch [200/782] Discriminator loss: 0.6880 Generator loss: 0.8086\n",
      "Epoch [20/500] Batch [300/782] Discriminator loss: 0.7031 Generator loss: 0.7492\n",
      "Epoch [20/500] Batch [400/782] Discriminator loss: 0.6727 Generator loss: 0.8069\n",
      "Epoch [20/500] Batch [500/782] Discriminator loss: 0.6711 Generator loss: 0.7623\n",
      "Epoch [20/500] Batch [600/782] Discriminator loss: 0.6612 Generator loss: 0.7274\n",
      "Epoch [20/500] Batch [700/782] Discriminator loss: 0.6603 Generator loss: 0.8306\n",
      "Epoch [21/500] Batch [100/782] Discriminator loss: 0.6768 Generator loss: 0.7300\n",
      "Epoch [21/500] Batch [200/782] Discriminator loss: 0.6848 Generator loss: 0.7525\n",
      "Epoch [21/500] Batch [300/782] Discriminator loss: 0.6880 Generator loss: 0.6896\n",
      "Epoch [21/500] Batch [400/782] Discriminator loss: 0.6632 Generator loss: 0.7966\n",
      "Epoch [21/500] Batch [500/782] Discriminator loss: 0.6659 Generator loss: 0.7635\n",
      "Epoch [21/500] Batch [600/782] Discriminator loss: 0.6826 Generator loss: 0.7026\n",
      "Epoch [21/500] Batch [700/782] Discriminator loss: 0.6935 Generator loss: 0.7380\n",
      "Epoch [22/500] Batch [100/782] Discriminator loss: 0.6793 Generator loss: 0.7550\n",
      "Epoch [22/500] Batch [200/782] Discriminator loss: 0.6847 Generator loss: 0.7569\n",
      "Epoch [22/500] Batch [300/782] Discriminator loss: 0.6699 Generator loss: 0.8467\n",
      "Epoch [22/500] Batch [400/782] Discriminator loss: 0.6657 Generator loss: 0.7012\n",
      "Epoch [22/500] Batch [500/782] Discriminator loss: 0.6809 Generator loss: 0.7991\n",
      "Epoch [22/500] Batch [600/782] Discriminator loss: 0.6794 Generator loss: 0.7440\n",
      "Epoch [22/500] Batch [700/782] Discriminator loss: 0.6646 Generator loss: 0.7627\n",
      "Epoch [23/500] Batch [100/782] Discriminator loss: 0.7231 Generator loss: 0.7217\n",
      "Epoch [23/500] Batch [200/782] Discriminator loss: 0.6730 Generator loss: 0.8522\n",
      "Epoch [23/500] Batch [300/782] Discriminator loss: 0.6989 Generator loss: 0.7598\n",
      "Epoch [23/500] Batch [400/782] Discriminator loss: 0.6879 Generator loss: 0.7827\n",
      "Epoch [23/500] Batch [500/782] Discriminator loss: 0.6949 Generator loss: 0.7532\n",
      "Epoch [23/500] Batch [600/782] Discriminator loss: 0.7235 Generator loss: 0.7334\n",
      "Epoch [23/500] Batch [700/782] Discriminator loss: 0.6821 Generator loss: 0.7171\n",
      "Epoch [24/500] Batch [100/782] Discriminator loss: 0.6797 Generator loss: 0.7210\n",
      "Epoch [24/500] Batch [200/782] Discriminator loss: 0.6791 Generator loss: 0.7746\n",
      "Epoch [24/500] Batch [300/782] Discriminator loss: 0.6881 Generator loss: 0.6555\n",
      "Epoch [24/500] Batch [400/782] Discriminator loss: 0.6868 Generator loss: 0.7691\n",
      "Epoch [24/500] Batch [500/782] Discriminator loss: 0.7126 Generator loss: 0.7527\n",
      "Epoch [24/500] Batch [600/782] Discriminator loss: 0.6715 Generator loss: 0.7266\n",
      "Epoch [24/500] Batch [700/782] Discriminator loss: 0.6559 Generator loss: 0.7013\n",
      "Epoch [25/500] Batch [100/782] Discriminator loss: 0.6868 Generator loss: 0.6961\n",
      "Epoch [25/500] Batch [200/782] Discriminator loss: 0.7007 Generator loss: 0.7177\n",
      "Epoch [25/500] Batch [300/782] Discriminator loss: 0.7173 Generator loss: 0.6894\n",
      "Epoch [25/500] Batch [400/782] Discriminator loss: 0.6752 Generator loss: 0.8113\n",
      "Epoch [25/500] Batch [500/782] Discriminator loss: 0.6511 Generator loss: 0.8171\n",
      "Epoch [25/500] Batch [600/782] Discriminator loss: 0.6706 Generator loss: 0.6703\n",
      "Epoch [25/500] Batch [700/782] Discriminator loss: 0.6752 Generator loss: 0.7648\n",
      "Epoch [26/500] Batch [100/782] Discriminator loss: 0.7254 Generator loss: 0.7059\n",
      "Epoch [26/500] Batch [200/782] Discriminator loss: 0.6953 Generator loss: 0.7697\n",
      "Epoch [26/500] Batch [300/782] Discriminator loss: 0.6918 Generator loss: 0.7399\n",
      "Epoch [26/500] Batch [400/782] Discriminator loss: 0.6758 Generator loss: 0.7183\n",
      "Epoch [26/500] Batch [500/782] Discriminator loss: 0.7053 Generator loss: 0.6910\n",
      "Epoch [26/500] Batch [600/782] Discriminator loss: 0.6971 Generator loss: 0.7373\n",
      "Epoch [26/500] Batch [700/782] Discriminator loss: 0.7248 Generator loss: 0.7590\n",
      "Epoch [27/500] Batch [100/782] Discriminator loss: 0.7071 Generator loss: 0.7649\n",
      "Epoch [27/500] Batch [200/782] Discriminator loss: 0.6878 Generator loss: 0.6981\n",
      "Epoch [27/500] Batch [300/782] Discriminator loss: 0.6953 Generator loss: 0.7278\n",
      "Epoch [27/500] Batch [400/782] Discriminator loss: 0.7015 Generator loss: 0.6939\n",
      "Epoch [27/500] Batch [500/782] Discriminator loss: 0.6653 Generator loss: 0.7330\n",
      "Epoch [27/500] Batch [600/782] Discriminator loss: 0.6507 Generator loss: 0.7095\n",
      "Epoch [27/500] Batch [700/782] Discriminator loss: 0.6980 Generator loss: 0.7331\n",
      "Epoch [28/500] Batch [100/782] Discriminator loss: 0.6879 Generator loss: 0.7111\n",
      "Epoch [28/500] Batch [200/782] Discriminator loss: 0.6937 Generator loss: 0.7626\n",
      "Epoch [28/500] Batch [300/782] Discriminator loss: 0.6792 Generator loss: 0.8051\n",
      "Epoch [28/500] Batch [400/782] Discriminator loss: 0.6947 Generator loss: 0.7551\n",
      "Epoch [28/500] Batch [500/782] Discriminator loss: 0.6992 Generator loss: 0.7572\n",
      "Epoch [28/500] Batch [600/782] Discriminator loss: 0.6914 Generator loss: 0.8013\n",
      "Epoch [28/500] Batch [700/782] Discriminator loss: 0.6941 Generator loss: 0.7182\n",
      "Epoch [29/500] Batch [100/782] Discriminator loss: 0.6829 Generator loss: 0.7720\n",
      "Epoch [29/500] Batch [200/782] Discriminator loss: 0.7135 Generator loss: 0.7400\n",
      "Epoch [29/500] Batch [300/782] Discriminator loss: 0.6785 Generator loss: 0.7828\n",
      "Epoch [29/500] Batch [400/782] Discriminator loss: 0.6906 Generator loss: 0.7101\n",
      "Epoch [29/500] Batch [500/782] Discriminator loss: 0.6870 Generator loss: 0.7163\n",
      "Epoch [29/500] Batch [600/782] Discriminator loss: 0.6887 Generator loss: 0.6777\n",
      "Epoch [29/500] Batch [700/782] Discriminator loss: 0.7005 Generator loss: 0.7709\n",
      "Epoch [30/500] Batch [100/782] Discriminator loss: 0.6917 Generator loss: 0.7710\n",
      "Epoch [30/500] Batch [200/782] Discriminator loss: 0.6895 Generator loss: 0.7004\n",
      "Epoch [30/500] Batch [300/782] Discriminator loss: 0.6953 Generator loss: 0.7726\n",
      "Epoch [30/500] Batch [400/782] Discriminator loss: 0.6824 Generator loss: 0.7494\n",
      "Epoch [30/500] Batch [500/782] Discriminator loss: 0.6753 Generator loss: 0.7625\n",
      "Epoch [30/500] Batch [600/782] Discriminator loss: 0.6847 Generator loss: 0.7457\n",
      "Epoch [30/500] Batch [700/782] Discriminator loss: 0.6810 Generator loss: 0.7090\n",
      "Epoch [31/500] Batch [100/782] Discriminator loss: 0.6638 Generator loss: 0.7718\n",
      "Epoch [31/500] Batch [200/782] Discriminator loss: 0.6908 Generator loss: 0.6920\n",
      "Epoch [31/500] Batch [300/782] Discriminator loss: 0.6851 Generator loss: 0.6717\n",
      "Epoch [31/500] Batch [400/782] Discriminator loss: 0.7160 Generator loss: 0.7254\n",
      "Epoch [31/500] Batch [500/782] Discriminator loss: 0.6978 Generator loss: 0.7271\n",
      "Epoch [31/500] Batch [600/782] Discriminator loss: 0.7047 Generator loss: 0.7354\n",
      "Epoch [31/500] Batch [700/782] Discriminator loss: 0.6946 Generator loss: 0.7264\n",
      "Epoch [32/500] Batch [100/782] Discriminator loss: 0.7105 Generator loss: 0.6994\n",
      "Epoch [32/500] Batch [200/782] Discriminator loss: 0.6742 Generator loss: 0.6992\n",
      "Epoch [32/500] Batch [300/782] Discriminator loss: 0.6800 Generator loss: 0.7546\n",
      "Epoch [32/500] Batch [400/782] Discriminator loss: 0.6863 Generator loss: 0.7723\n",
      "Epoch [32/500] Batch [500/782] Discriminator loss: 0.7117 Generator loss: 0.7052\n",
      "Epoch [32/500] Batch [600/782] Discriminator loss: 0.7085 Generator loss: 0.7356\n",
      "Epoch [32/500] Batch [700/782] Discriminator loss: 0.6955 Generator loss: 0.7140\n",
      "Epoch [33/500] Batch [100/782] Discriminator loss: 0.6833 Generator loss: 0.7482\n",
      "Epoch [33/500] Batch [200/782] Discriminator loss: 0.6848 Generator loss: 0.7687\n",
      "Epoch [33/500] Batch [300/782] Discriminator loss: 0.7247 Generator loss: 0.7031\n",
      "Epoch [33/500] Batch [400/782] Discriminator loss: 0.6701 Generator loss: 0.8114\n",
      "Epoch [33/500] Batch [500/782] Discriminator loss: 0.6862 Generator loss: 0.7232\n",
      "Epoch [33/500] Batch [600/782] Discriminator loss: 0.6545 Generator loss: 0.6856\n",
      "Epoch [33/500] Batch [700/782] Discriminator loss: 0.6985 Generator loss: 0.7126\n",
      "Epoch [34/500] Batch [100/782] Discriminator loss: 0.6910 Generator loss: 0.6847\n",
      "Epoch [34/500] Batch [200/782] Discriminator loss: 0.7169 Generator loss: 0.7187\n",
      "Epoch [34/500] Batch [300/782] Discriminator loss: 0.7109 Generator loss: 0.7589\n",
      "Epoch [34/500] Batch [400/782] Discriminator loss: 0.6884 Generator loss: 0.7359\n",
      "Epoch [34/500] Batch [500/782] Discriminator loss: 0.6753 Generator loss: 0.6898\n",
      "Epoch [34/500] Batch [600/782] Discriminator loss: 0.6663 Generator loss: 0.7554\n",
      "Epoch [34/500] Batch [700/782] Discriminator loss: 0.6932 Generator loss: 0.6936\n",
      "Epoch [35/500] Batch [100/782] Discriminator loss: 0.6922 Generator loss: 0.7142\n",
      "Epoch [35/500] Batch [200/782] Discriminator loss: 0.6781 Generator loss: 0.8055\n",
      "Epoch [35/500] Batch [300/782] Discriminator loss: 0.6776 Generator loss: 0.7699\n",
      "Epoch [35/500] Batch [400/782] Discriminator loss: 0.6714 Generator loss: 0.7178\n",
      "Epoch [35/500] Batch [500/782] Discriminator loss: 0.6898 Generator loss: 0.8026\n",
      "Epoch [35/500] Batch [600/782] Discriminator loss: 0.7135 Generator loss: 0.7558\n",
      "Epoch [35/500] Batch [700/782] Discriminator loss: 0.6963 Generator loss: 0.7244\n",
      "Epoch [36/500] Batch [100/782] Discriminator loss: 0.7084 Generator loss: 0.7032\n",
      "Epoch [36/500] Batch [200/782] Discriminator loss: 0.6842 Generator loss: 0.7449\n",
      "Epoch [36/500] Batch [300/782] Discriminator loss: 0.7041 Generator loss: 0.6688\n",
      "Epoch [36/500] Batch [400/782] Discriminator loss: 0.7159 Generator loss: 0.7007\n",
      "Epoch [36/500] Batch [500/782] Discriminator loss: 0.6934 Generator loss: 0.6827\n",
      "Epoch [36/500] Batch [600/782] Discriminator loss: 0.6823 Generator loss: 0.7073\n",
      "Epoch [36/500] Batch [700/782] Discriminator loss: 0.6879 Generator loss: 0.7001\n",
      "Epoch [37/500] Batch [100/782] Discriminator loss: 0.6787 Generator loss: 0.7354\n",
      "Epoch [37/500] Batch [200/782] Discriminator loss: 0.6822 Generator loss: 0.7110\n",
      "Epoch [37/500] Batch [300/782] Discriminator loss: 0.6883 Generator loss: 0.7061\n",
      "Epoch [37/500] Batch [400/782] Discriminator loss: 0.6941 Generator loss: 0.8355\n",
      "Epoch [37/500] Batch [500/782] Discriminator loss: 0.6858 Generator loss: 0.8115\n",
      "Epoch [37/500] Batch [600/782] Discriminator loss: 0.6845 Generator loss: 0.7458\n",
      "Epoch [37/500] Batch [700/782] Discriminator loss: 0.6879 Generator loss: 0.7584\n",
      "Epoch [38/500] Batch [100/782] Discriminator loss: 0.7018 Generator loss: 0.6992\n",
      "Epoch [38/500] Batch [200/782] Discriminator loss: 0.6989 Generator loss: 0.6936\n",
      "Epoch [38/500] Batch [300/782] Discriminator loss: 0.6992 Generator loss: 0.6774\n",
      "Epoch [38/500] Batch [400/782] Discriminator loss: 0.6873 Generator loss: 0.7877\n",
      "Epoch [38/500] Batch [500/782] Discriminator loss: 0.6851 Generator loss: 0.7465\n",
      "Epoch [38/500] Batch [600/782] Discriminator loss: 0.6937 Generator loss: 0.7177\n",
      "Epoch [38/500] Batch [700/782] Discriminator loss: 0.7090 Generator loss: 0.6926\n",
      "Epoch [39/500] Batch [100/782] Discriminator loss: 0.6848 Generator loss: 0.7653\n",
      "Epoch [39/500] Batch [200/782] Discriminator loss: 0.6963 Generator loss: 0.7526\n",
      "Epoch [39/500] Batch [300/782] Discriminator loss: 0.6814 Generator loss: 0.7767\n",
      "Epoch [39/500] Batch [400/782] Discriminator loss: 0.6767 Generator loss: 0.8220\n",
      "Epoch [39/500] Batch [500/782] Discriminator loss: 0.6845 Generator loss: 0.7637\n",
      "Epoch [39/500] Batch [600/782] Discriminator loss: 0.6871 Generator loss: 0.6986\n",
      "Epoch [39/500] Batch [700/782] Discriminator loss: 0.6914 Generator loss: 0.7481\n",
      "Epoch [40/500] Batch [100/782] Discriminator loss: 0.6884 Generator loss: 0.7613\n",
      "Epoch [40/500] Batch [200/782] Discriminator loss: 0.6919 Generator loss: 0.6987\n",
      "Epoch [40/500] Batch [300/782] Discriminator loss: 0.7032 Generator loss: 0.7579\n",
      "Epoch [40/500] Batch [400/782] Discriminator loss: 0.6852 Generator loss: 0.7638\n",
      "Epoch [40/500] Batch [500/782] Discriminator loss: 0.7120 Generator loss: 0.7251\n",
      "Epoch [40/500] Batch [600/782] Discriminator loss: 0.6917 Generator loss: 0.7538\n",
      "Epoch [40/500] Batch [700/782] Discriminator loss: 0.7090 Generator loss: 0.6834\n",
      "Epoch [41/500] Batch [100/782] Discriminator loss: 0.6911 Generator loss: 0.6998\n",
      "Epoch [41/500] Batch [200/782] Discriminator loss: 0.6933 Generator loss: 0.7104\n",
      "Epoch [41/500] Batch [300/782] Discriminator loss: 0.7111 Generator loss: 0.7332\n",
      "Epoch [41/500] Batch [400/782] Discriminator loss: 0.6755 Generator loss: 0.7137\n",
      "Epoch [41/500] Batch [500/782] Discriminator loss: 0.7378 Generator loss: 0.8507\n",
      "Epoch [41/500] Batch [600/782] Discriminator loss: 0.7239 Generator loss: 0.6929\n",
      "Epoch [41/500] Batch [700/782] Discriminator loss: 0.6967 Generator loss: 0.7543\n",
      "Epoch [42/500] Batch [100/782] Discriminator loss: 0.6765 Generator loss: 0.7259\n",
      "Epoch [42/500] Batch [200/782] Discriminator loss: 0.6958 Generator loss: 0.7016\n",
      "Epoch [42/500] Batch [300/782] Discriminator loss: 0.7025 Generator loss: 0.6998\n",
      "Epoch [42/500] Batch [400/782] Discriminator loss: 0.6668 Generator loss: 0.6919\n",
      "Epoch [42/500] Batch [500/782] Discriminator loss: 0.6644 Generator loss: 0.7462\n",
      "Epoch [42/500] Batch [600/782] Discriminator loss: 0.7079 Generator loss: 0.7090\n",
      "Epoch [42/500] Batch [700/782] Discriminator loss: 0.7059 Generator loss: 0.6939\n",
      "Epoch [43/500] Batch [100/782] Discriminator loss: 0.6828 Generator loss: 0.7689\n",
      "Epoch [43/500] Batch [200/782] Discriminator loss: 0.7423 Generator loss: 0.7389\n",
      "Epoch [43/500] Batch [300/782] Discriminator loss: 0.6861 Generator loss: 0.7205\n",
      "Epoch [43/500] Batch [400/782] Discriminator loss: 0.6770 Generator loss: 0.6847\n",
      "Epoch [43/500] Batch [500/782] Discriminator loss: 0.6816 Generator loss: 0.6853\n",
      "Epoch [43/500] Batch [600/782] Discriminator loss: 0.6793 Generator loss: 0.7052\n",
      "Epoch [43/500] Batch [700/782] Discriminator loss: 0.6986 Generator loss: 0.7080\n",
      "Epoch [44/500] Batch [100/782] Discriminator loss: 0.6947 Generator loss: 0.7508\n",
      "Epoch [44/500] Batch [200/782] Discriminator loss: 0.6872 Generator loss: 0.7163\n",
      "Epoch [44/500] Batch [300/782] Discriminator loss: 0.6735 Generator loss: 0.7205\n",
      "Epoch [44/500] Batch [400/782] Discriminator loss: 0.6790 Generator loss: 0.7911\n",
      "Epoch [44/500] Batch [500/782] Discriminator loss: 0.6970 Generator loss: 0.7278\n",
      "Epoch [44/500] Batch [600/782] Discriminator loss: 0.6819 Generator loss: 0.7048\n",
      "Epoch [44/500] Batch [700/782] Discriminator loss: 0.6959 Generator loss: 0.7426\n",
      "Epoch [45/500] Batch [100/782] Discriminator loss: 0.6663 Generator loss: 0.6853\n",
      "Epoch [45/500] Batch [200/782] Discriminator loss: 0.6858 Generator loss: 0.7042\n",
      "Epoch [45/500] Batch [300/782] Discriminator loss: 0.7183 Generator loss: 0.7029\n",
      "Epoch [45/500] Batch [400/782] Discriminator loss: 0.7003 Generator loss: 0.7359\n",
      "Epoch [45/500] Batch [500/782] Discriminator loss: 0.7102 Generator loss: 0.7029\n",
      "Epoch [45/500] Batch [600/782] Discriminator loss: 0.7008 Generator loss: 0.7452\n",
      "Epoch [45/500] Batch [700/782] Discriminator loss: 0.6841 Generator loss: 0.7843\n",
      "Epoch [46/500] Batch [100/782] Discriminator loss: 0.7119 Generator loss: 0.7371\n",
      "Epoch [46/500] Batch [200/782] Discriminator loss: 0.6797 Generator loss: 0.7370\n",
      "Epoch [46/500] Batch [300/782] Discriminator loss: 0.6906 Generator loss: 0.7055\n",
      "Epoch [46/500] Batch [400/782] Discriminator loss: 0.7139 Generator loss: 0.7053\n",
      "Epoch [46/500] Batch [500/782] Discriminator loss: 0.6673 Generator loss: 0.7550\n",
      "Epoch [46/500] Batch [600/782] Discriminator loss: 0.6897 Generator loss: 0.7060\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14000\\4203546833.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \"\"\"\n\u001b[0;32m     26\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatent_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mgen_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \"\"\"\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14000\\2984289185.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\test\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generator와 Discriminator를 번갈아 학습합니다.\n",
    "\"\"\"\n",
    "g_loss_list = []\n",
    "d_loss_list = []\n",
    "for epoch in tqdm(range(config.n_epoch)):\n",
    "    for i, (real_img, _) in enumerate(train_loader):\n",
    "        \n",
    "        real_img = real_img.to(config.device)\n",
    "\n",
    "        \"\"\"\n",
    "        adversarial loss에 사용될 ground truth들입니다.\n",
    "        Discriminator에게 있어 실제 이미지는 1, generator가 생성한 fake 이미지는 0을 label로 합니다.\n",
    "        반대로 Generator는 자신이 생성한 fake 이미지의 label이 1이 되게 하여 Discriminator를 fooling 합니다.\n",
    "        \"\"\"\n",
    "        valid_label = torch.ones((real_img.shape[0], 1), device=config.device, dtype=torch.float32)\n",
    "        fake_label = torch.zeros((real_img.shape[0], 1), device=config.device, dtype=torch.float32)\n",
    "        \n",
    "        # ====================================================#\n",
    "        #                Train Discriminator                  #\n",
    "        # ====================================================#\n",
    "\n",
    "        \"\"\"\n",
    "        Gaussian random noise를 Generator에게 입력하여 fake 이미지들을 생성합니다.\n",
    "        \"\"\"\n",
    "        z = torch.randn((real_img.shape[0], config.latent_size), device=config.device, dtype=torch.float32)\n",
    "        gen_img = generator(z)\n",
    "\n",
    "        \"\"\"\n",
    "        Discriminator가 실제 이미지와 Generator가 생성한 이미지를 잘 구별하는지 loss를 계산합니다.\n",
    "        이 때, Generator는 현재 계산된 loss로 학습되지 않으므로, \n",
    "        detach() 함수를 이용하여 생성 이미지를 computation graph에서 분리한 후 Discriminator의 입력으로 넣어줍니다. \n",
    "        \"\"\"\n",
    "        real_loss = criterion(discriminator(real_img), valid_label)\n",
    "        fake_loss = criterion(discriminator(gen_img.detach()), fake_label)\n",
    "        d_loss = (real_loss + fake_loss) * 0.5\n",
    "        \n",
    "        \"\"\"\n",
    "        Discriminator를 업데이트합니다.\n",
    "        \"\"\"\n",
    "        optimizer_d.zero_grad()\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # ====================================================#\n",
    "        #                   Train Generator                   #\n",
    "        # ====================================================#\n",
    "\n",
    "        \"\"\"\n",
    "        Gaussian random noise를 Generator에게 입력하여 fake 이미지들을 생성합니다.\n",
    "        \"\"\"\n",
    "        z = torch.randn((real_img.shape[0], config.latent_size), device=config.device, dtype=torch.float32)\n",
    "        gen_img = generator(z)\n",
    "\n",
    "        \"\"\"\n",
    "        Generator가 Discriminator를 속일 수 있는지 loss를 계산합니다.\n",
    "        \"\"\"\n",
    "        g_loss = criterion(discriminator(gen_img), valid_label)\n",
    "        \n",
    "        \"\"\"\n",
    "        Generator를 업데이트합니다.\n",
    "        \"\"\"\n",
    "        optimizer_g.zero_grad()\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        if (i+1) % config.log_interval == 0:\n",
    "            g_loss_list.append(g_loss.item())\n",
    "            d_loss_list.append(d_loss.item())\n",
    "            print('Epoch [{}/{}] Batch [{}/{}] Discriminator loss: {:.4f} Generator loss: {:.4f}'.format(\n",
    "                epoch+1, config.n_epoch, i+1, len(train_loader), d_loss.item(), g_loss.item()))\n",
    "\n",
    "    if (epoch+1) % config.save_interval == 0:\n",
    "        save_path = os.path.join(config.save_path, config.dataset, 'epoch_[{}].png'.format(epoch+1))\n",
    "        gen_img = config.denormalize(gen_img)\n",
    "        torchvision.utils.save_image(gen_img.data[:25], save_path, nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3227398701.py, line 48)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\fishd\\AppData\\Local\\Temp\\ipykernel_10780\\3227398701.py\"\u001b[1;36m, line \u001b[1;32m48\u001b[0m\n\u001b[1;33m    - [torch.nn.BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)\u001b[0m\n\u001b[1;37m                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "plt.title('GAN training loss on {} data'.format(config.dataset))\n",
    "plt.plot(g_loss_list, label='generator loss')\n",
    "plt.plot(d_loss_list, label='discriminator loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "## Qualitative results\n",
    "save_path = os.path.join(config.save_path, config.dataset)\n",
    "for image_path in os.listdir(save_path):\n",
    "    if image_path.endswith('.png'):\n",
    "        plt.figure(figsize=(5,5))\n",
    "        image = Image.open(os.path.join(save_path, image_path))\n",
    "        plt.title(image_path)\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7940d0d72e881cb11dd568ba28aec900f56fa18be17fd320ef03619339573588"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('normal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
